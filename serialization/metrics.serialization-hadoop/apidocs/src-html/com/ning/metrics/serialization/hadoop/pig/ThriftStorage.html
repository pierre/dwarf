<HTML>
<BODY BGCOLOR="white">
<PRE>
<FONT color="green">001</FONT>    package com.ning.metrics.serialization.hadoop.pig;<a name="line.1"></a>
<FONT color="green">002</FONT>    <a name="line.2"></a>
<FONT color="green">003</FONT>    import com.ning.metrics.goodwill.access.GoodwillAccessor;<a name="line.3"></a>
<FONT color="green">004</FONT>    import com.ning.metrics.goodwill.access.GoodwillSchema;<a name="line.4"></a>
<FONT color="green">005</FONT>    import com.ning.metrics.goodwill.access.GoodwillSchemaField;<a name="line.5"></a>
<FONT color="green">006</FONT>    import com.ning.metrics.serialization.hadoop.HadoopThriftEnvelopeSerialization;<a name="line.6"></a>
<FONT color="green">007</FONT>    import com.ning.metrics.serialization.hadoop.HadoopThriftWritableSerialization;<a name="line.7"></a>
<FONT color="green">008</FONT>    import com.ning.metrics.serialization.schema.SchemaFieldType;<a name="line.8"></a>
<FONT color="green">009</FONT>    import com.ning.metrics.serialization.thrift.ThriftEnvelope;<a name="line.9"></a>
<FONT color="green">010</FONT>    import com.ning.metrics.serialization.thrift.ThriftField;<a name="line.10"></a>
<FONT color="green">011</FONT>    import com.ning.metrics.serialization.thrift.hadoop.ThriftWritable;<a name="line.11"></a>
<FONT color="green">012</FONT>    import com.ning.metrics.serialization.thrift.item.DataItem;<a name="line.12"></a>
<FONT color="green">013</FONT>    import org.apache.hadoop.conf.Configuration;<a name="line.13"></a>
<FONT color="green">014</FONT>    import org.apache.hadoop.mapreduce.InputFormat;<a name="line.14"></a>
<FONT color="green">015</FONT>    import org.apache.hadoop.mapreduce.Job;<a name="line.15"></a>
<FONT color="green">016</FONT>    import org.apache.hadoop.mapreduce.RecordReader;<a name="line.16"></a>
<FONT color="green">017</FONT>    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<a name="line.17"></a>
<FONT color="green">018</FONT>    import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;<a name="line.18"></a>
<FONT color="green">019</FONT>    import org.apache.log4j.Logger;<a name="line.19"></a>
<FONT color="green">020</FONT>    import org.apache.pig.Expression;<a name="line.20"></a>
<FONT color="green">021</FONT>    import org.apache.pig.LoadFunc;<a name="line.21"></a>
<FONT color="green">022</FONT>    import org.apache.pig.LoadMetadata;<a name="line.22"></a>
<FONT color="green">023</FONT>    import org.apache.pig.ResourceSchema;<a name="line.23"></a>
<FONT color="green">024</FONT>    import org.apache.pig.ResourceStatistics;<a name="line.24"></a>
<FONT color="green">025</FONT>    import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;<a name="line.25"></a>
<FONT color="green">026</FONT>    import org.apache.pig.data.DataType;<a name="line.26"></a>
<FONT color="green">027</FONT>    import org.apache.pig.data.Tuple;<a name="line.27"></a>
<FONT color="green">028</FONT>    import org.apache.pig.data.TupleFactory;<a name="line.28"></a>
<FONT color="green">029</FONT>    import org.apache.pig.impl.logicalLayer.schema.Schema;<a name="line.29"></a>
<FONT color="green">030</FONT>    <a name="line.30"></a>
<FONT color="green">031</FONT>    import java.io.IOException;<a name="line.31"></a>
<FONT color="green">032</FONT>    import java.util.ArrayList;<a name="line.32"></a>
<FONT color="green">033</FONT>    import java.util.List;<a name="line.33"></a>
<FONT color="green">034</FONT>    import java.util.concurrent.ExecutionException;<a name="line.34"></a>
<FONT color="green">035</FONT>    <a name="line.35"></a>
<FONT color="green">036</FONT>    public class ThriftStorage extends LoadFunc implements LoadMetadata<a name="line.36"></a>
<FONT color="green">037</FONT>    {<a name="line.37"></a>
<FONT color="green">038</FONT>        private static final Logger log = Logger.getLogger(ThriftStorage.class);<a name="line.38"></a>
<FONT color="green">039</FONT>    <a name="line.39"></a>
<FONT color="green">040</FONT>        private final TupleFactory factory = TupleFactory.getInstance();<a name="line.40"></a>
<FONT color="green">041</FONT>        private final GoodwillSchema schema;<a name="line.41"></a>
<FONT color="green">042</FONT>    <a name="line.42"></a>
<FONT color="green">043</FONT>        private Object value;<a name="line.43"></a>
<FONT color="green">044</FONT>        private RecordReader reader;<a name="line.44"></a>
<FONT color="green">045</FONT>        private PigSplit split;<a name="line.45"></a>
<FONT color="green">046</FONT>    <a name="line.46"></a>
<FONT color="green">047</FONT>        public ThriftStorage(String schemaName) throws IOException<a name="line.47"></a>
<FONT color="green">048</FONT>        {<a name="line.48"></a>
<FONT color="green">049</FONT>            this(schemaName, System.getProperty("goodwill.host", "127.0.0.1"), Integer.getInteger("goodwill.port", 8080));<a name="line.49"></a>
<FONT color="green">050</FONT>        }<a name="line.50"></a>
<FONT color="green">051</FONT>    <a name="line.51"></a>
<FONT color="green">052</FONT>        public ThriftStorage(String schemaName, String goodwillHost, int goodwillPort) throws IOException<a name="line.52"></a>
<FONT color="green">053</FONT>        {<a name="line.53"></a>
<FONT color="green">054</FONT>            try {<a name="line.54"></a>
<FONT color="green">055</FONT>                GoodwillAccessor goodwillAccessor = new GoodwillAccessor(goodwillHost, goodwillPort);<a name="line.55"></a>
<FONT color="green">056</FONT>                schema = goodwillAccessor.getSchema(schemaName).get();<a name="line.56"></a>
<FONT color="green">057</FONT>    <a name="line.57"></a>
<FONT color="green">058</FONT>                if (schema == null) {<a name="line.58"></a>
<FONT color="green">059</FONT>                    throw new IOException(String.format("Unable to find schema %s in Goodwill (%s:%d)",<a name="line.59"></a>
<FONT color="green">060</FONT>                        schemaName, goodwillHost, goodwillPort));<a name="line.60"></a>
<FONT color="green">061</FONT>                }<a name="line.61"></a>
<FONT color="green">062</FONT>            }<a name="line.62"></a>
<FONT color="green">063</FONT>            catch (InterruptedException e) {<a name="line.63"></a>
<FONT color="green">064</FONT>                throw new IOException("Interrupted while trying to fetch Thrift schema", e);<a name="line.64"></a>
<FONT color="green">065</FONT>            }<a name="line.65"></a>
<FONT color="green">066</FONT>            catch (ExecutionException e) {<a name="line.66"></a>
<FONT color="green">067</FONT>                throw new IOException("Exception while trying to fetch Thrfit schema", e);<a name="line.67"></a>
<FONT color="green">068</FONT>            }<a name="line.68"></a>
<FONT color="green">069</FONT>        }<a name="line.69"></a>
<FONT color="green">070</FONT>    <a name="line.70"></a>
<FONT color="green">071</FONT>        /**<a name="line.71"></a>
<FONT color="green">072</FONT>         * Communicate to the loader the location of the object(s) being loaded.<a name="line.72"></a>
<FONT color="green">073</FONT>         * The location string passed to the LoadFunc here is the return value of<a name="line.73"></a>
<FONT color="green">074</FONT>         * {@link org.apache.pig.LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}. Implementations<a name="line.74"></a>
<FONT color="green">075</FONT>         * should use this method to communicate the location (and any other information)<a name="line.75"></a>
<FONT color="green">076</FONT>         * to its underlying InputFormat through the Job object.<a name="line.76"></a>
<FONT color="green">077</FONT>         * &lt;p/&gt;<a name="line.77"></a>
<FONT color="green">078</FONT>         * This method will be called in the backend multiple times. Implementations<a name="line.78"></a>
<FONT color="green">079</FONT>         * should bear in mind that this method is called multiple times and should<a name="line.79"></a>
<FONT color="green">080</FONT>         * ensure there are no inconsistent side effects due to the multiple calls.<a name="line.80"></a>
<FONT color="green">081</FONT>         *<a name="line.81"></a>
<FONT color="green">082</FONT>         * @param location Location as returned by<a name="line.82"></a>
<FONT color="green">083</FONT>         *                 {@link org.apache.pig.LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}<a name="line.83"></a>
<FONT color="green">084</FONT>         * @param job      the {@link org.apache.hadoop.mapreduce.Job} object<a name="line.84"></a>
<FONT color="green">085</FONT>         *                 store or retrieve earlier stored information from the {@link org.apache.pig.impl.util.UDFContext}<a name="line.85"></a>
<FONT color="green">086</FONT>         * @throws java.io.IOException if the location is not valid.<a name="line.86"></a>
<FONT color="green">087</FONT>         */<a name="line.87"></a>
<FONT color="green">088</FONT>        @Override<a name="line.88"></a>
<FONT color="green">089</FONT>        public void setLocation(String location, Job job) throws IOException<a name="line.89"></a>
<FONT color="green">090</FONT>        {<a name="line.90"></a>
<FONT color="green">091</FONT>            setIOSerializations(job.getConfiguration());<a name="line.91"></a>
<FONT color="green">092</FONT>            FileInputFormat.setInputPaths(job, location);<a name="line.92"></a>
<FONT color="green">093</FONT>        }<a name="line.93"></a>
<FONT color="green">094</FONT>    <a name="line.94"></a>
<FONT color="green">095</FONT>        /**<a name="line.95"></a>
<FONT color="green">096</FONT>         * This will be called during planning on the front end. This is the<a name="line.96"></a>
<FONT color="green">097</FONT>         * instance of InputFormat (rather than the class name) because the<a name="line.97"></a>
<FONT color="green">098</FONT>         * load function may need to instantiate the InputFormat in order<a name="line.98"></a>
<FONT color="green">099</FONT>         * to control how it is constructed.<a name="line.99"></a>
<FONT color="green">100</FONT>         *<a name="line.100"></a>
<FONT color="green">101</FONT>         * @return the InputFormat associated with this loader.<a name="line.101"></a>
<FONT color="green">102</FONT>         * @throws java.io.IOException if there is an exception during InputFormat<a name="line.102"></a>
<FONT color="green">103</FONT>         *                             construction<a name="line.103"></a>
<FONT color="green">104</FONT>         */<a name="line.104"></a>
<FONT color="green">105</FONT>        @Override<a name="line.105"></a>
<FONT color="green">106</FONT>        public InputFormat getInputFormat() throws IOException<a name="line.106"></a>
<FONT color="green">107</FONT>        {<a name="line.107"></a>
<FONT color="green">108</FONT>            return new SequenceFileInputFormat&lt;ThriftWritable, ThriftEnvelope&gt;();<a name="line.108"></a>
<FONT color="green">109</FONT>        }<a name="line.109"></a>
<FONT color="green">110</FONT>    <a name="line.110"></a>
<FONT color="green">111</FONT>        /**<a name="line.111"></a>
<FONT color="green">112</FONT>         * Initializes LoadFunc for reading data.  This will be called during execution<a name="line.112"></a>
<FONT color="green">113</FONT>         * before any calls to getNext.  The RecordReader needs to be passed here because<a name="line.113"></a>
<FONT color="green">114</FONT>         * it has been instantiated for a particular InputSplit.<a name="line.114"></a>
<FONT color="green">115</FONT>         *<a name="line.115"></a>
<FONT color="green">116</FONT>         * @param reader {@link org.apache.hadoop.mapreduce.RecordReader} to be used by this instance of the LoadFunc<a name="line.116"></a>
<FONT color="green">117</FONT>         * @param split  The input {@link org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit} to process<a name="line.117"></a>
<FONT color="green">118</FONT>         * @throws java.io.IOException if there is an exception during initialization<a name="line.118"></a>
<FONT color="green">119</FONT>         */<a name="line.119"></a>
<FONT color="green">120</FONT>        @Override<a name="line.120"></a>
<FONT color="green">121</FONT>        public void prepareToRead(RecordReader reader, PigSplit split) throws IOException<a name="line.121"></a>
<FONT color="green">122</FONT>        {<a name="line.122"></a>
<FONT color="green">123</FONT>            this.reader = reader;<a name="line.123"></a>
<FONT color="green">124</FONT>            this.split = split;<a name="line.124"></a>
<FONT color="green">125</FONT>            setIOSerializations(split.getConf());<a name="line.125"></a>
<FONT color="green">126</FONT>        }<a name="line.126"></a>
<FONT color="green">127</FONT>    <a name="line.127"></a>
<FONT color="green">128</FONT>        private void setIOSerializations(Configuration conf)<a name="line.128"></a>
<FONT color="green">129</FONT>        {<a name="line.129"></a>
<FONT color="green">130</FONT>            String[] configuredSerializations = conf.getStrings("io.serializations");<a name="line.130"></a>
<FONT color="green">131</FONT>            int i = configuredSerializations.length;<a name="line.131"></a>
<FONT color="green">132</FONT>    <a name="line.132"></a>
<FONT color="green">133</FONT>            String[] allSerializations = new String[i + 3];<a name="line.133"></a>
<FONT color="green">134</FONT>            System.arraycopy(configuredSerializations, 0, allSerializations, 0, i);<a name="line.134"></a>
<FONT color="green">135</FONT>    <a name="line.135"></a>
<FONT color="green">136</FONT>            allSerializations[i] = HadoopThriftWritableSerialization.class.getName();<a name="line.136"></a>
<FONT color="green">137</FONT>            allSerializations[i + 1] = HadoopThriftEnvelopeSerialization.class.getName();<a name="line.137"></a>
<FONT color="green">138</FONT>            allSerializations[i + 2] = "org.apache.hadoop.io.serializer.WritableSerialization";<a name="line.138"></a>
<FONT color="green">139</FONT>    <a name="line.139"></a>
<FONT color="green">140</FONT>            conf.setStrings("io.serializations", allSerializations);<a name="line.140"></a>
<FONT color="green">141</FONT>        }<a name="line.141"></a>
<FONT color="green">142</FONT>    <a name="line.142"></a>
<FONT color="green">143</FONT>        @Override<a name="line.143"></a>
<FONT color="green">144</FONT>        public Tuple getNext() throws IOException<a name="line.144"></a>
<FONT color="green">145</FONT>        {<a name="line.145"></a>
<FONT color="green">146</FONT>            try {<a name="line.146"></a>
<FONT color="green">147</FONT>                List&lt;Object&gt; tupleList = new ArrayList&lt;Object&gt;();<a name="line.147"></a>
<FONT color="green">148</FONT>    <a name="line.148"></a>
<FONT color="green">149</FONT>                if (reader == null || !reader.nextKeyValue()) {<a name="line.149"></a>
<FONT color="green">150</FONT>                    return null;<a name="line.150"></a>
<FONT color="green">151</FONT>                }<a name="line.151"></a>
<FONT color="green">152</FONT>    <a name="line.152"></a>
<FONT color="green">153</FONT>                value = reader.getCurrentValue();<a name="line.153"></a>
<FONT color="green">154</FONT>    <a name="line.154"></a>
<FONT color="green">155</FONT>                if (value instanceof ThriftEnvelope) {<a name="line.155"></a>
<FONT color="green">156</FONT>                    ThriftEnvelope envelope = (ThriftEnvelope) value;<a name="line.156"></a>
<FONT color="green">157</FONT>                    Tuple tuple = factory.newTuple(envelope.getPayload().size());<a name="line.157"></a>
<FONT color="green">158</FONT>                    for (ThriftField thriftField : envelope.getPayload()) {<a name="line.158"></a>
<FONT color="green">159</FONT>                        GoodwillSchemaField schemaField = schema.getFieldByPosition(thriftField.getId());<a name="line.159"></a>
<FONT color="green">160</FONT>    <a name="line.160"></a>
<FONT color="green">161</FONT>                        if (schemaField == null) {<a name="line.161"></a>
<FONT color="green">162</FONT>                            throw new IOException(String.format("got a thrift ID [%d] that is not part of the schema", thriftField.getId()));<a name="line.162"></a>
<FONT color="green">163</FONT>                        }<a name="line.163"></a>
<FONT color="green">164</FONT>    <a name="line.164"></a>
<FONT color="green">165</FONT>                        // Thrift starts at 1<a name="line.165"></a>
<FONT color="green">166</FONT>                        tuple.set(thriftField.getId() - 1, convertToObject(thriftField.getDataItem(), schemaField.getType()));<a name="line.166"></a>
<FONT color="green">167</FONT>                    }<a name="line.167"></a>
<FONT color="green">168</FONT>    <a name="line.168"></a>
<FONT color="green">169</FONT>                    return tuple;<a name="line.169"></a>
<FONT color="green">170</FONT>                }<a name="line.170"></a>
<FONT color="green">171</FONT>                else {<a name="line.171"></a>
<FONT color="green">172</FONT>                    throw new IOException(String.format("Expected ThriftEnvelope, not %s", value.getClass()));<a name="line.172"></a>
<FONT color="green">173</FONT>                }<a name="line.173"></a>
<FONT color="green">174</FONT>            }<a name="line.174"></a>
<FONT color="green">175</FONT>            catch (InterruptedException e) {<a name="line.175"></a>
<FONT color="green">176</FONT>                log.warn("Interrupted getting next tuple", e);<a name="line.176"></a>
<FONT color="green">177</FONT>            }<a name="line.177"></a>
<FONT color="green">178</FONT>    <a name="line.178"></a>
<FONT color="green">179</FONT>            return null;<a name="line.179"></a>
<FONT color="green">180</FONT>        }<a name="line.180"></a>
<FONT color="green">181</FONT>    <a name="line.181"></a>
<FONT color="green">182</FONT>        private Object convertToObject(DataItem dataItem, SchemaFieldType type)<a name="line.182"></a>
<FONT color="green">183</FONT>        {<a name="line.183"></a>
<FONT color="green">184</FONT>            switch (type) {<a name="line.184"></a>
<FONT color="green">185</FONT>                // Pig does not support a boolean type (yet), so convert to integer<a name="line.185"></a>
<FONT color="green">186</FONT>                case BOOLEAN:<a name="line.186"></a>
<FONT color="green">187</FONT>                    return dataItem.getBoolean() ? 1 : 0;<a name="line.187"></a>
<FONT color="green">188</FONT>                case BYTE:<a name="line.188"></a>
<FONT color="green">189</FONT>                    return dataItem.getByte();<a name="line.189"></a>
<FONT color="green">190</FONT>                case SHORT:<a name="line.190"></a>
<FONT color="green">191</FONT>                case INTEGER:<a name="line.191"></a>
<FONT color="green">192</FONT>                    return dataItem.getInteger();<a name="line.192"></a>
<FONT color="green">193</FONT>                case LONG:<a name="line.193"></a>
<FONT color="green">194</FONT>                    return dataItem.getLong();<a name="line.194"></a>
<FONT color="green">195</FONT>                case DOUBLE:<a name="line.195"></a>
<FONT color="green">196</FONT>                    return dataItem.getDouble();<a name="line.196"></a>
<FONT color="green">197</FONT>                case DATE:<a name="line.197"></a>
<FONT color="green">198</FONT>                case IP:<a name="line.198"></a>
<FONT color="green">199</FONT>                case STRING:<a name="line.199"></a>
<FONT color="green">200</FONT>                default:<a name="line.200"></a>
<FONT color="green">201</FONT>                    return dataItem.getString();<a name="line.201"></a>
<FONT color="green">202</FONT>            }<a name="line.202"></a>
<FONT color="green">203</FONT>        }<a name="line.203"></a>
<FONT color="green">204</FONT>    <a name="line.204"></a>
<FONT color="green">205</FONT>        private byte getPigType(SchemaFieldType type)<a name="line.205"></a>
<FONT color="green">206</FONT>        {<a name="line.206"></a>
<FONT color="green">207</FONT>            switch (type) {<a name="line.207"></a>
<FONT color="green">208</FONT>                case BOOLEAN:<a name="line.208"></a>
<FONT color="green">209</FONT>                    return DataType.INTEGER;<a name="line.209"></a>
<FONT color="green">210</FONT>                case BYTE:<a name="line.210"></a>
<FONT color="green">211</FONT>                    return DataType.BYTE;<a name="line.211"></a>
<FONT color="green">212</FONT>                case SHORT:<a name="line.212"></a>
<FONT color="green">213</FONT>                case INTEGER:<a name="line.213"></a>
<FONT color="green">214</FONT>                    return DataType.INTEGER;<a name="line.214"></a>
<FONT color="green">215</FONT>                case LONG:<a name="line.215"></a>
<FONT color="green">216</FONT>                    return DataType.LONG;<a name="line.216"></a>
<FONT color="green">217</FONT>                case DOUBLE:<a name="line.217"></a>
<FONT color="green">218</FONT>                    return DataType.DOUBLE;<a name="line.218"></a>
<FONT color="green">219</FONT>                case DATE:<a name="line.219"></a>
<FONT color="green">220</FONT>                case IP:<a name="line.220"></a>
<FONT color="green">221</FONT>                case STRING:<a name="line.221"></a>
<FONT color="green">222</FONT>                default:<a name="line.222"></a>
<FONT color="green">223</FONT>                    return DataType.CHARARRAY;<a name="line.223"></a>
<FONT color="green">224</FONT>            }<a name="line.224"></a>
<FONT color="green">225</FONT>        }<a name="line.225"></a>
<FONT color="green">226</FONT>    <a name="line.226"></a>
<FONT color="green">227</FONT>        /**<a name="line.227"></a>
<FONT color="green">228</FONT>         * Get a schema for the data to be loaded.<a name="line.228"></a>
<FONT color="green">229</FONT>         *<a name="line.229"></a>
<FONT color="green">230</FONT>         * @param location Location as returned by<a name="line.230"></a>
<FONT color="green">231</FONT>         *                 {@link org.apache.pig.LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}<a name="line.231"></a>
<FONT color="green">232</FONT>         * @param job      The {@link org.apache.hadoop.mapreduce.Job} object - this should be used only to obtain<a name="line.232"></a>
<FONT color="green">233</FONT>         *                 cluster properties through {@link org.apache.hadoop.mapreduce.Job#getConfiguration()} and not to set/query<a name="line.233"></a>
<FONT color="green">234</FONT>         *                 any runtime job information.<a name="line.234"></a>
<FONT color="green">235</FONT>         * @return schema for the data to be loaded. This schema should represent<a name="line.235"></a>
<FONT color="green">236</FONT>         *         all tuples of the returned data.  If the schema is unknown or it is<a name="line.236"></a>
<FONT color="green">237</FONT>         *         not possible to return a schema that represents all returned data,<a name="line.237"></a>
<FONT color="green">238</FONT>         *         then null should be returned. The schema should not be affected by pushProjection, ie.<a name="line.238"></a>
<FONT color="green">239</FONT>         *         getSchema should always return the original schema even after pushProjection<a name="line.239"></a>
<FONT color="green">240</FONT>         * @throws java.io.IOException if an exception occurs while determining the schema<a name="line.240"></a>
<FONT color="green">241</FONT>         */<a name="line.241"></a>
<FONT color="green">242</FONT>        @Override<a name="line.242"></a>
<FONT color="green">243</FONT>        public ResourceSchema getSchema(String location, Job job) throws IOException<a name="line.243"></a>
<FONT color="green">244</FONT>        {<a name="line.244"></a>
<FONT color="green">245</FONT>            List&lt;Schema.FieldSchema&gt; schemaList = new ArrayList&lt;Schema.FieldSchema&gt;();<a name="line.245"></a>
<FONT color="green">246</FONT>            for (GoodwillSchemaField field : schema.getSchema()) {<a name="line.246"></a>
<FONT color="green">247</FONT>                schemaList.add(new Schema.FieldSchema(field.getName(), getPigType(field.getType())));<a name="line.247"></a>
<FONT color="green">248</FONT>            }<a name="line.248"></a>
<FONT color="green">249</FONT>    <a name="line.249"></a>
<FONT color="green">250</FONT>            return new ResourceSchema(new Schema(schemaList));<a name="line.250"></a>
<FONT color="green">251</FONT>        }<a name="line.251"></a>
<FONT color="green">252</FONT>    <a name="line.252"></a>
<FONT color="green">253</FONT>        /**<a name="line.253"></a>
<FONT color="green">254</FONT>         * Get statistics about the data to be loaded.  If no statistics are<a name="line.254"></a>
<FONT color="green">255</FONT>         * available, then null should be returned.<a name="line.255"></a>
<FONT color="green">256</FONT>         *<a name="line.256"></a>
<FONT color="green">257</FONT>         * @param location Location as returned by<a name="line.257"></a>
<FONT color="green">258</FONT>         *                 {@link org.apache.pig.LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}<a name="line.258"></a>
<FONT color="green">259</FONT>         * @param job      The {@link org.apache.hadoop.mapreduce.Job} object - this should be used only to obtain<a name="line.259"></a>
<FONT color="green">260</FONT>         *                 cluster properties through {@link org.apache.hadoop.mapreduce.Job#getConfiguration()} and not to set/query<a name="line.260"></a>
<FONT color="green">261</FONT>         *                 any runtime job information.<a name="line.261"></a>
<FONT color="green">262</FONT>         * @return statistics about the data to be loaded.  If no statistics are<a name="line.262"></a>
<FONT color="green">263</FONT>         *         available, then null should be returned.<a name="line.263"></a>
<FONT color="green">264</FONT>         * @throws java.io.IOException if an exception occurs while retrieving statistics<a name="line.264"></a>
<FONT color="green">265</FONT>         */<a name="line.265"></a>
<FONT color="green">266</FONT>        @Override<a name="line.266"></a>
<FONT color="green">267</FONT>        public ResourceStatistics getStatistics(String location, Job job) throws IOException<a name="line.267"></a>
<FONT color="green">268</FONT>        {<a name="line.268"></a>
<FONT color="green">269</FONT>            return null;<a name="line.269"></a>
<FONT color="green">270</FONT>        }<a name="line.270"></a>
<FONT color="green">271</FONT>    <a name="line.271"></a>
<FONT color="green">272</FONT>        /**<a name="line.272"></a>
<FONT color="green">273</FONT>         * Find what columns are partition keys for this input.<a name="line.273"></a>
<FONT color="green">274</FONT>         *<a name="line.274"></a>
<FONT color="green">275</FONT>         * @param location Location as returned by<a name="line.275"></a>
<FONT color="green">276</FONT>         *                 {@link org.apache.pig.LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}<a name="line.276"></a>
<FONT color="green">277</FONT>         * @param job      The {@link org.apache.hadoop.mapreduce.Job} object - this should be used only to obtain<a name="line.277"></a>
<FONT color="green">278</FONT>         *                 cluster properties through {@link org.apache.hadoop.mapreduce.Job#getConfiguration()} and not to set/query<a name="line.278"></a>
<FONT color="green">279</FONT>         *                 any runtime job information.<a name="line.279"></a>
<FONT color="green">280</FONT>         * @return array of field names of the partition keys. Implementations<a name="line.280"></a>
<FONT color="green">281</FONT>         *         should return null to indicate that there are no partition keys<a name="line.281"></a>
<FONT color="green">282</FONT>         * @throws java.io.IOException if an exception occurs while retrieving partition keys<a name="line.282"></a>
<FONT color="green">283</FONT>         */<a name="line.283"></a>
<FONT color="green">284</FONT>        @Override<a name="line.284"></a>
<FONT color="green">285</FONT>        public String[] getPartitionKeys(String location, Job job) throws IOException<a name="line.285"></a>
<FONT color="green">286</FONT>        {<a name="line.286"></a>
<FONT color="green">287</FONT>            return null;<a name="line.287"></a>
<FONT color="green">288</FONT>        }<a name="line.288"></a>
<FONT color="green">289</FONT>    <a name="line.289"></a>
<FONT color="green">290</FONT>        /**<a name="line.290"></a>
<FONT color="green">291</FONT>         * Set the filter for partitioning.  It is assumed that this filter<a name="line.291"></a>
<FONT color="green">292</FONT>         * will only contain references to fields given as partition keys in<a name="line.292"></a>
<FONT color="green">293</FONT>         * getPartitionKeys. So if the implementation returns null in<a name="line.293"></a>
<FONT color="green">294</FONT>         * {@link #getPartitionKeys(String, org.apache.hadoop.mapreduce.Job)}, then this method is not<a name="line.294"></a>
<FONT color="green">295</FONT>         * called by Pig runtime. This method is also not called by the Pig runtime<a name="line.295"></a>
<FONT color="green">296</FONT>         * if there are no partition filter conditions.<a name="line.296"></a>
<FONT color="green">297</FONT>         *<a name="line.297"></a>
<FONT color="green">298</FONT>         * @param partitionFilter that describes filter for partitioning<a name="line.298"></a>
<FONT color="green">299</FONT>         * @throws java.io.IOException if the filter is not compatible with the storage<a name="line.299"></a>
<FONT color="green">300</FONT>         *                             mechanism or contains non-partition fields.<a name="line.300"></a>
<FONT color="green">301</FONT>         */<a name="line.301"></a>
<FONT color="green">302</FONT>        @Override<a name="line.302"></a>
<FONT color="green">303</FONT>        public void setPartitionFilter(Expression partitionFilter) throws IOException<a name="line.303"></a>
<FONT color="green">304</FONT>        {<a name="line.304"></a>
<FONT color="green">305</FONT>        }<a name="line.305"></a>
<FONT color="green">306</FONT>    }<a name="line.306"></a>




























































</PRE>
</BODY>
</HTML>
